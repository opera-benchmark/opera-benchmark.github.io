(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[450],{4491:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog/RespLLM",function(){return n(7607)}])},9095:function(e,t,n){"use strict";n.d(t,{Z:function(){return o}});var a=n(4637),i=n(5565),s=n.n(i);function o(e){var t=e.src,n=e.alt;return(0,a.jsx)(s(),{src:t,alt:n,width:"0",height:"0",sizes:"100vw",className:"w-full h-auto"})}},7607:function(e,t,n){"use strict";n.r(t);var a=n(8961),i=n(7835),s=n(4637),o=n(5445),r=n(8850),l=n(4319),d=n(4879),p=(n(94),n(5565),n(9095)),u=[{name:"blog",children:[{name:"RespLLM",route:"/blog/RespLLM"},{name:"meta.json",meta:{overview:"OPERA Overview",RespLLM:"RespLLM"}},{name:"overview",route:"/blog/overview"}],route:"/blog"},{name:"index",route:"/",frontMatter:{title:"OPERA"}},{name:"meta.json",meta:{index:{title:"Introduction",type:"page",hidden:!0},blog:{title:"Blog",type:"page"}}}];globalThis.__nextra_internal__={pageMap:u,route:"/blog/RespLLM"};var c=(0,r.withSSG)((0,o.Z)({filename:"RespLLM.mdx",route:"/blog/RespLLM",meta:{},pageMap:u,titleText:"RespLLM",headings:[{type:"heading",depth:1,children:[{type:"text",value:"RespLLM",position:{start:{line:4,column:3,offset:90},end:{line:4,column:10,offset:97}}}],position:{start:{line:4,column:1,offset:88},end:{line:4,column:10,offset:97}},value:"RespLLM"}],hasH1:!0},l.Z));function h(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},t=function(){var t=Object.assign({h1:"h1",p:"p"},(0,d.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{children:"RespLLM"}),"\n",(0,s.jsx)(t.p,{children:"The high incidence and mortality rates associated with respiratory diseases underscores the importance of early screening. Machine learning models can automate clinical consultations and auscultation, offering vital support in this area. However, the data involved, spanning demographics, medical history, symptoms, and respiratory audio, are heterogeneous and complex. Existing approaches are insufficient and lack generalizability, as they typically rely on limited training data, basic fusion techniques, and task-specific models."}),"\n",(0,s.jsx)(t.p,{children:"We propose RespLLM, a novel multimodal large language model (LLM) framework that unifies text and audio representations for respiratory health prediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs and enables effective audio-text fusion through cross-modal attentions. Instruction tuning is employed to integrate diverse data from multiple sources, ensuring generalizability and versatility of the model."}),"\n",(0,s.jsx)(p.Z,{src:"/images/RespLLM.png",alt:"System overview of RespLLM"}),"\n",(0,s.jsx)(t.p,{children:"Experiments on five real-world datasets demonstrate that RespLLM outperforms leading baselines by an average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates zero-shot predictions for new tasks. Our work lays the foundation for multimodal models that can perceive, listen to, and understand heterogeneous data, paving the way for scalable respiratory health diagnosis."})]})},n=Object.assign({},(0,d.ah)(),e.components),o=n.wrapper;return o?(0,s.jsx)(o,(0,i.Z)((0,a.Z)({},e),{children:(0,s.jsx)(t,{})})):t()}function f(e){return(0,s.jsx)(c,(0,i.Z)((0,a.Z)({},e),{children:(0,s.jsx)(h,{})}))}f.getLayout=c.getLayout,t.default=f}},function(e){e.O(0,[294,565,319,774,888,179],(function(){return t=4491,e(e.s=t);var t}));var t=e.O();_N_E=t}]);