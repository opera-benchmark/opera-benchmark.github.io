(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[405],{7681:function(e,t,r){(window.__NEXT_P=window.__NEXT_P||[]).push(["/",function(){return r(9257)}])},9257:function(e,t,r){"use strict";r.r(t),r.d(t,{default:function(){return j}});var n=r(8961),a=r(7835),i=r(4637),o=r(5445),s=r(8850),l=r(4319),d=r(4879),c=r(94),h=r.n(c),u=r(9496),g={default:"bg-orange-50 border border-orange-100 text-orange-800 dark:text-orange-300 dark:bg-orange-400 dark:border-orange-400 dark:bg-opacity-20 dark:border-opacity-30",error:"bg-red-100 border border-red-200 text-red-900 dark:text-red-200 dark:bg-red-900 dark:bg-opacity-30 dark:border-opacity-30",info:"bg-blue-100 border border-blue-200 text-blue-900 dark:text-blue-200 dark:bg-blue-900 dark:bg-opacity-30 dark:border-opacity-30",warning:"bg-yellow-50 border border-yellow-100 text-yellow-900 dark:text-yellow-200 dark:bg-yellow-700 dark:bg-opacity-30"},p=({children:e,type:t="default",emoji:r="\ud83d\udca1"})=>u.createElement("div",{className:`${g[t]} flex rounded-lg nextra-callout mt-6`},u.createElement("div",{className:"pl-3 pr-2 py-2 select-none text-xl",style:{fontFamily:'"Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"'}},r),u.createElement("div",{className:"pr-4 py-2"},e)),m=(r(9803),r(1484),[{name:"blog",children:[{name:"RespLLM",route:"/blog/RespLLM"},{name:"meta.json",meta:{overview:"OPERA Overview",RespLLM:"RespLLM"}},{name:"overview",route:"/blog/overview"}],route:"/blog"},{name:"index",route:"/",frontMatter:{title:"OPERA"}},{name:"meta.json",meta:{index:{title:"Introduction",type:"page",hidden:!0},blog:{title:"Blog",type:"page"}}}]);globalThis.__nextra_internal__={pageMap:m,route:"/"};var x=(0,s.withSSG)((0,o.Z)({filename:"index.mdx",route:"/",meta:{title:"OPERA"},pageMap:m,titleText:"Supported by",headings:[{type:"heading",depth:1,children:[{type:"text",value:"Supported by",position:{start:{line:39,column:3,offset:1870},end:{line:39,column:15,offset:1882}}}],position:{start:{line:39,column:1,offset:1868},end:{line:39,column:15,offset:1882}},value:"Supported by"},{type:"heading",depth:2,children:[{type:"text",value:"Publications",position:{start:{line:48,column:4,offset:2441},end:{line:48,column:16,offset:2453}}}],position:{start:{line:48,column:1,offset:2438},end:{line:48,column:16,offset:2453}},value:"Publications"}],hasH1:!0},l.Z));function b(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},t=function(){var t=Object.assign({h1:"h1",p:"p",strong:"strong",ul:"ul",li:"li",a:"a",h2:"h2",em:"em"},(0,d.ah)(),e.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(i.Fragment,{children:(0,i.jsx)(t.h1,{className:"text-center font-extrabold md:text-5xl mt-8",children:(0,i.jsx)(h(),{src:"/images/OPERA_logo.png",alt:"OPERA Logo",height:100,width:333})})}),"\n",(0,i.jsx)("div",{className:"mx-auto max-w-full w-[880px] text-center px-4 mb-10",children:(0,i.jsx)("div",{className:"text-lg mb-2 text-gray-600 md:!text-2xl",children:(0,i.jsxs)(t.p,{children:["the first ",(0,i.jsx)("u",{children:(0,i.jsx)("strong",{children:"OPE"})}),"n  ",(0,i.jsx)("u",{children:(0,i.jsx)("strong",{children:"R"})}),"espiratory ",(0,i.jsx)("u",{children:(0,i.jsx)("strong",{children:"A"})}),"coustic foundation model\npretraining and benchmarking system"]})})}),"\n",(0,i.jsx)(p,{emoji:"",children:(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"OPERA"})," is a system that curates ",(0,i.jsx)(t.strong,{children:"large-scale unlabelled"})," respiratory audio datasets to pretrain audio encoders that are ",(0,i.jsx)(t.strong,{children:"generalizable"})," to be adapted for various health tasks with ",(0,i.jsx)(t.strong,{children:"limited labeled data"}),"."]})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"OPERA"})," system allows us to:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Curate a unique large-scale(~136K samples, 400+ hours), multi-source (5 datasets), multi-modal (breathing, coughing, and lung sounds) and publicly available (or under controlled access) dataset for model pretraining."}),"\n",(0,i.jsx)(t.li,{children:"Pretrain 3 generalizable acoustic models with the curated unlabeled data using contrastive learning and generative pretraining, and release the model checkpoints."}),"\n",(0,i.jsx)(t.li,{children:"Employ 10 labeled datasets (6 not covered by pretraining) to formulate 19 respiratory health tasks,  ensuring fair, comprehensive and reproducible downstream evaluation."}),"\n",(0,i.jsx)(t.li,{children:"Enable researchers and developers to extract feature using our model, or develop new models with our data and system, as a starting point for future exploration."}),"\n"]}),"\n",(0,i.jsx)("div",{className:"mt-16 mb-20 text-center",children:(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://arxiv.org/pdf/2406.16148",children:"Paper"})," \xb7 ",(0,i.jsx)(t.a,{href:"/blog/overview",children:"Blog"})," \xb7 ",(0,i.jsx)(t.a,{href:"https://github.com/evelyn0414/OPERA",children:"GitHub Repository"})]})}),"\n",(0,i.jsxs)("div",{className:"mt-16 mb-20 text-center",children:[(0,i.jsx)(t.h1,{children:"Supported by"}),(0,i.jsxs)(t.p,{children:[(0,i.jsx)(h(),{src:"/images/clab.png",alt:"cam",height:250,width:250}),"\xa0\xa0\xa0\xa0\xa0\xa0\xa0",(0,i.jsx)(h(),{src:"/images/erc.png",alt:"erc",height:250,width:250})]}),(0,i.jsx)(h(),{src:"/images/EPSRC_logo.png",alt:"epsrc",height:80,width:320}),(0,i.jsx)(t.p,{children:"The study has been approved by the Ethics Committee of the Department of Computer Science and Technology, University of Cambridge, and is partly funded by the European Research Council through Project EAR and by the Engineering and Physical Sciences Research Council through Project Reload."})]}),"\n",(0,i.jsx)(t.h2,{id:"publications",children:"Publications"}),"\n",(0,i.jsx)(t.p,{children:"Check the paper introducing OPERA datasets, pretrained models and the benchmark."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Zhang Y, Xia T, Han J, Wu Y, Rizos G, Liu Y, Mosuily M, Chauhan J, Mascolo C. Towards open respiratory acoustic foundation models: Pretraining and benchmarking. In ",(0,i.jsx)(t.em,{children:"Thirty-eighth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"}),", 2024."]}),"\n"]})]})},r=Object.assign({},(0,d.ah)(),e.components),o=r.wrapper;return o?(0,i.jsx)(o,(0,a.Z)((0,n.Z)({},e),{children:(0,i.jsx)(t,{})})):t()}function f(e){return(0,i.jsx)(x,(0,a.Z)((0,n.Z)({},e),{children:(0,i.jsx)(b,{})}))}f.getLayout=x.getLayout;var j=f}},function(e){e.O(0,[294,319,774,888,179],(function(){return t=7681,e(e.s=t);var t}));var t=e.O();_N_E=t}]);
